# -*- coding: utf-8 -*-
"""2020451132_hw3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lw5kdjTUTBlebLH9Z0h1Z4lJ_ry841Cf

<h1>Exercise 1</h1>

(1) improvement 적용 전
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Dense, BatchNormalization, Dropout
from tensorflow.keras import optimizers
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.ensemble import VotingClassifier

whole_data = load_breast_cancer()

X_data = whole_data.data
y_data = whole_data.target

X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.3, random_state = 42)
print(y_test)
print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

model = Sequential()

model.add(Dense(10, input_shape = (30,), activation = 'sigmoid'))
model.add(Dense(10, activation = 'sigmoid'))
model.add(Dense(10, activation = 'sigmoid'))
model.add(Dense(1, activation = 'sigmoid'))
model.summary()

sgd = optimizers.SGD(lr = 0.01)    # stochastic gradient descent optimizer
model.compile(optimizer = sgd, loss = 'binary_crossentropy', metrics = ['accuracy'])

history = model.fit(X_train, y_train, batch_size = 50, epochs = 300, validation_split = 0.2, verbose = 1)

import matplotlib.pyplot as plt

plt.plot(history.history['loss'])
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy / Loss')
plt.xlabel('Epoch')
plt.legend(['loss', 'test_acc', 'val_acc'], loc='upper right')
plt.show()

results = model.evaluate(X_test, y_test)

print('loss: ', results[0])
print('accuracy: ', results[1])

"""(2) Improvement 적용 후"""

whole_data = load_breast_cancer()

X_data1 = whole_data.data
y_data1 = whole_data.target

X_train1, X_test1, y_train1, y_test1 = train_test_split(X_data1, y_data1, test_size = 0.3, random_state = 42)
print(y_test1)
print(X_train1.shape, y_train1.shape, X_test1.shape, y_test1.shape)

def model_imp() :
    model = Sequential()

    model.add(Dense(10, input_shape = (30,), kernel_initializer='he_normal', activation = 'relu'))
    model.add(BatchNormalization())
    model.add(Dense(10, kernel_initializer='he_normal',activation = 'relu'))
    model.add(BatchNormalization())
    model.add(Dense(10, kernel_initializer='he_normal',activation = 'relu'))
    model.add(BatchNormalization())
    model.add(Dense(1, kernel_initializer='he_normal', activation = 'relu'))

    adam = optimizers.Adam(lr = 0.01)
    model.compile(optimizer = adam, loss = 'binary_crossentropy', metrics = ['accuracy'])

    return model

model1 = model_imp()
history = model1.fit(X_train1, y_train1, batch_size = 50, epochs = 150, validation_split = 0.2, verbose = 1)

plt.plot(history.history['loss'])
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy / Loss')
plt.xlabel('Epoch')
plt.legend(['loss', 'test_acc', 'val_acc'], loc='upper right')
plt.show()

results1 = model1.evaluate(X_test1, y_test1)

print('test accuracy: ', results1[1])

"""- 결론 : epoch를 줄이고 relu activation, batch 정규화, adam 최적화 기법을 사용했더니 accuracy가 63%에서 약 96%까지 상승했다.

<br>
<br>
<h1>Exercise 2</h1>
"""

from sklearn.datasets import load_iris
data = load_iris()
X_data = data.data
y_data = data.target

X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.3, random_state=42)

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

print(data.DESCR)

unique, counts = np.unique(y_train, return_counts=True)
print("Train labels: ", dict(zip(unique, counts)))

print(X_train[:3,], y_train[:3])

"""(1) 기본 : sigmoid activation | dense = 10 | random initialize | SGD optimizer"""

def model1():
    model = Sequential()
    model.add(Dense(10, input_shape = (4, ), activation='sigmoid')) 
    model.add(Dense(10, activation='sigmoid'))
    model.add(Dense(10, activation='sigmoid'))
    model.add(Dense(1, activation='softmax'))

    sgd = optimizers.SGD(lr = 0.001)  
    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])

    return model

model1 = model1()
history = model1.fit(X_train, y_train, batch_size = 10, validation_split = 0.3, epochs = 100, verbose = 1)

import matplotlib.pyplot as plt

plt.plot(history.history['loss'])
plt.plot(history.history['accuracy'])
#plt.plot(history.history['acc'])
#plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy / Loss')
plt.xlabel('Epoch')
plt.legend(['Loss', 'Test Accuracy'], loc='upper right')
plt.show()

results = model1.evaluate(X_test, y_test)
print('Test accuracy: ', results[1])

"""(2) activation : relu activation | dense = 10 | he_normal initialize | SGD optimizer"""

def model2():
    model = Sequential()
    model.add(Dense(10, input_shape = (4, ), activation='relu', kernel_initializer='he_normal')) 
    model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(1, activation='sigmoid'))

    sgd = optimizers.SGD(lr = 0.001)  
    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])

    return model

model2 = model2()
history = model2.fit(X_train, y_train, batch_size = 10, validation_split = 0.3, epochs = 100, verbose = 1)

import matplotlib.pyplot as plt

plt.plot(history.history['loss'])
plt.plot(history.history['accuracy'])
#plt.plot(history.history['acc'])
#plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy / Loss')
plt.xlabel('Epoch')
plt.legend(['Loss', 'Test Accuracy'], loc='upper right')
plt.show()

results = model2.evaluate(X_test, y_test)
print('Test accuracy: ', results[1])

"""(3) activation : relu activation | dense = 10 | he_normal initialize | Adam optimizer"""

def model3():
    model = Sequential()
    model.add(Dense(10, input_shape = (4, ), activation='relu', kernel_initializer='he_normal')) 
    model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(1, activation='sigmoid'))

    adam = optimizers.Adam(lr = 0.001)  
    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])

    return model

model3 = model3()
history = model3.fit(X_train, y_train, batch_size = 10, validation_split = 0.3, epochs = 100, verbose = 1)

results = model2.evaluate(X_test, y_test)
print('Test accuracy: ', results[1])

"""(4) activation : relu activation | dense = 10 | he_normal initialize | Adam optimizer | Batch Normalization"""

def model4():
    model = Sequential()
    model.add(Dense(10, input_shape = (4, ), activation='relu', kernel_initializer='he_normal')) 
    model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(1, activation='sigmoid'))

    adam = optimizers.Adam(lr = 0.001)  
    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])

    model = Sequential()
    model.add(Dense(10, input_shape = (4, ), kernel_initializer='he_normal'))
    model.add(BatchNormalization())
    model.add(Activation('relu'))     
    model.add(Dense(10, kernel_initializer='he_normal'))
    model.add(BatchNormalization())
    model.add(Activation('relu'))      
    model.add(Dense(10, kernel_initializer='he_normal'))
    model.add(BatchNormalization())
    model.add(Activation('relu'))    
    model.add(Dense(1, kernel_initializer='he_normal'))
    model.add(Activation('softmax'))

    adam = optimizers.Adam(lr = 0.001)  
    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])

    return model

model4 = model4()
history = model4.fit(X_train, y_train, batch_size = 10, validation_split = 0.3, epochs = 100, verbose = 1)

results = model2.evaluate(X_test, y_test)
print('Test accuracy: ', results[1])

"""(5) activation : relu activation | dense = 20 | he_normal initialize | Adam optimizer | epoch = 200"""

def model5():
    model = Sequential()
    model.add(Dense(20, input_shape = (4, ), activation='relu', kernel_initializer='he_normal')) 
    model.add(Dense(20, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(20, activation='relu', kernel_initializer='he_normal'))
    model.add(Dense(1, activation='sigmoid'))

    adam = optimizers.Adam(lr = 0.001)  
    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])

    return model

model5 = model5()
history = model5.fit(X_train, y_train, batch_size = 10, validation_split = 0.3, epochs = 200, verbose = 1)

results = model2.evaluate(X_test, y_test)
print('Test accuracy: ', results[1])

"""- 결론 : 첫번째 모델보다 다른 모델의 test accuracy가 높았지만, 유의미하게 높아지지는 않았다. 모델 자체를 바꿔보는 것이 도움이 될 것 같다."""