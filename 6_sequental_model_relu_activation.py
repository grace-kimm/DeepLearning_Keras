# -*- coding: utf-8 -*-
"""2020451132_hw6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UBQrWKDI4r0YEQb8pdvtTlDjE8Zz7KjM

* 결론 : BatchNormalization과 weight initalization 적용으로 **87.39%에서 88.52%까지** 성능이 개선 되었다.
"""

import numpy as np
import matplotlib.pyplot as plt

from keras.datasets import imdb
from keras.preprocessing.sequence import pad_sequences

num_features = 3000
sequence_length = 300
embedding_dimension = 100

(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = num_features)

X_train = pad_sequences(X_train, maxlen = sequence_length)
X_test = pad_sequences(X_test, maxlen = sequence_length)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

print("X_train[0] : ", X_train[0])
print("X_train[0]'s length : ", len(X_train[0]))

print("X_train[1] : ", X_train[0])
print("X_train[1]'s length : ", len(X_train[1]))

print("X_train : ", X_train.shape, "X_test : ", X_test.shape,
      "y_train : ", y_train.shape,"y_test : ", y_test.shape)

"""1D convolution"""

from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Conv1D, MaxPooling1D, Embedding, Flatten
from keras import optimizers

def imdb_cnn() :
    model = Sequential()

    model.add(Embedding(input_dim=3000, 
                        output_dim=embedding_dimension,
                        input_length=sequence_length))
    
    model.add(Conv1D(filters=50, kernel_size=5, strides=1, padding='valid'))
    model.add(MaxPooling1D(2, padding = 'valid'))

    model.add(Flatten())

    model.add(Dense(10))
    model.add(Activation('relu'))
    model.add(Dense(1))
    model.add(Activation('sigmoid'))

    adam = optimizers.Adam(lr = 0.001)

    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])

    return model

model = imdb_cnn()

history = model.fit(X_train, y_train, batch_size=50, epochs=100, validation_split=0.2, verbose=0)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['training', 'validation'], loc = 'upper right')
plt.show()

results = model.evaluate(X_test, y_test)
print('Test accuracy : ', results[1])

"""2D convolution"""

def imdb_cnn_2() :
    model = Sequential()

    model.add(Embedding(input_dim=3000, 
                        output_dim=embedding_dimension,
                        input_length=sequence_length))
    model.add(Reshape((sequence_length, embedding_dimension, 1), 
                      input_shape=(sequence_length, embedding_dimension)))
    model.add(Conv2D(filters=50, kernel_size=(5,embedding_dimension), strides=(1,1), padding='valid'))
    model.add(MaxPooling2D())

    model.add(Dense(10))
    model.add(Activation('relu'))
    model.add(Dropout(0.3))
    model.add(Dense(10))
    model.add(Activation('relu'))
    model.add(Dropout(0.3))
    model.add(Dense(1))
    model.add(Activation('sigmoid'))

    adam = optimizers.Adam(lr = 0.001)

    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])

    return model

model = imdb_cnn()

history = model.fit(X_train, y_train, batch_size=50, epochs=100, validation_split=0.2, verbose=0)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['training', 'validation'], loc = 'upper right')
plt.show()

results = model.evaluate(X_test, y_test)
print('Test accuracy : ', results[1])

"""Different Filter Size : Functional API"""

from keras.models import Model
from keras.layers import concatenate, Input, Reshape, Conv2D, MaxPooling2D

filter_sizes = [3, 4, 5]

# 합성곱 연산을 위한 함수
def convolution() :
    inn = Input(shape = (sequence_length, embedding_dimension, 1))
    convolutions = []

    # conv + pooling을 3회 시행 후 결과를 concatenate
    for fs in filter_sizes :
        conv = Conv2D(filters=100, kernel_size=(fs, embedding_dimension),
                      strides=1, padding="valid")(inn)
        nonlinearity = Activation('relu')(conv)
        maxpool = MaxPooling2D(pool_size=(sequence_length - fs + 1, 1), 
                               padding="valid")(nonlinearity)
        convolutions.append(maxpool)

    outt = concatenate(convolutions)
    model = Model(inputs=inn, outputs=outt)

    return model

# CNN 모델
def imdb_cnn_3() :
    model = Sequential()
    model.add(Embedding(input_dim=3000, output_dim=embedding_dimension, input_length=sequence_length))
    model.add(Reshape((sequence_length, embedding_dimension, 1), 
                      input_shape = (sequence_length, embedding_dimension)))
    
    # convolution method 호출
    model.add(convolution())

    model.add(Flatten())
    model.add(Dense(10))
    model.add(Activation('relu'))
    model.add(Dropout(0.3))

    model.add(Dense(10))
    model.add(Activation('relu'))
    model.add(Dropout(0.3))

    model.add(Dense(1))
    model.add(Activation('sigmoid'))

    adam = optimizers.Adam(lr=0.001)

    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])

    return model

model = imdb_cnn_3()

history = model.fit(X_train, y_train, batch_size=50, epochs=100, validation_split=0.2, verbose=0)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['training', 'validation'], loc = 'upper right')
plt.show()

results = model.evaluate(X_test, y_test)
print('Test accuracy : ', results[1])

"""개선해보기 : batchNormalization, kernel_initializer"""

from keras.layers import BatchNormalization

filter_sizes = [3, 4, 5]

def convolution():
    inn = Input(shape = (sequence_length, embedding_dimension, 1))
    convolutions = []

    for fs in filter_sizes:
        conv = Conv2D(filters = 100, kernel_size = (fs, embedding_dimension), strides = 1, padding = "valid")(inn)
        nonlinearity = Activation('relu')(conv)
        maxpool = MaxPooling2D(pool_size = (sequence_length - fs + 1, 1), padding = "valid")(nonlinearity)
        convolutions.append(maxpool)

    outt = concatenate(convolutions)
    model = Model(inputs = inn, outputs = outt)

    return model

def imdb_cnn_4():
    
    model = Sequential()
    model.add(Embedding(input_dim = 3000, output_dim = embedding_dimension, input_length = sequence_length))
    model.add(Reshape((sequence_length, embedding_dimension, 1), input_shape = (sequence_length, embedding_dimension)))
    model.add(Dropout(0.5))

    model.add(convolution())
    
    model.add(Flatten())
    model.add(Dense(10, kernel_initializer='he_normal'))
    model.add(BatchNormalization())
    model.add(Activation('relu'))

    model.add(Dense(10, kernel_initializer='he_normal'))
    model.add(Activation('relu'))
    model.add(BatchNormalization())

    model.add(Dense(1))
    model.add(Activation('sigmoid'))

    adam = optimizers.Adam(lr = 0.001)

    model.compile(loss='binary_crossentropy', optimizer=adam , metrics=['accuracy'])
    
    return model

model = imdb_cnn_4()

history = model.fit(X_train, y_train, batch_size = 50, epochs = 100, validation_split = 0.2, verbose = 0)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['training', 'validation'], loc = 'upper right')
plt.show()

results = model.evaluate(X_test, y_test)
print('Test accuracy : ', results[1])